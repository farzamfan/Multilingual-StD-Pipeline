feature='dict_sentiment', count = 'tokens')
sent_counts
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
score_frequencies = table(pet_supplies$score)
barplot(score_frequencies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
data_dictionary_AFINN = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
View(data_dictionary_AFINN)
data_dictionary_AFINN$dict_sentiment = data_dictionary_AFINN$code
tc$code_dictionary(data_dictionary_AFINN, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = data_dictionary_AFINN$code
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
score_frequencies = table(pet_supplies$score)
barplot(score_frequencies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
View(AuthoritatianDict)
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
sent_counts$dict_score = log(sent_counts$positive + 0.5) - log(sent_counts$negative + 0.5)
hist(sent_counts$dict_score, xlab='score', main='dictionary sentiment score')
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
View(pet_supplies)
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
View(pet_supplies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
View(AuthoritatianDict)
typeof(AuthoritatianDict)
data_dictionary_AFINN = read_csv('~/VU Teaching/SMA 2021/data/AFINN_dictionary.csv')
data_dictionary_AFINN$dict_sentiment = data_dictionary_AFINN$code
typeof(data_dictionary_AFINN)
View(data_dictionary_AFINN)
data_dictionary_AFINN$dict_sentiment = data_dictionary_AFINN$code
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
View(data_dictionary_AFINN)
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id'),
feature='dict_sentiment', count = 'tokens')
sent_counts
tc
tc$code_dictionary
View(data_dictionary_AFINN)
summary(data_char_encodedtexts)
summary(data_dictionary_AFINN)
summary(AuthoritatianDict)
View(data_dictionary_AFINN)
View(AuthoritatianDict)
unique(AuthoritatianDict)
unique(AuthoritatianDict$code)
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')[50]
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')[50]
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')[50,]
View(AuthoritatianDict)
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')[1:50,]
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')[2:50,]
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
View(AuthoritatianDict)
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict[AuthoritatianDict$code === "aut",]
AuthoritatianDict[AuthoritatianDict$code == "aut",]
AuthoritatianDict[AuthoritatianDict$code != "aut",]
Temp <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
score_frequencies = table(pet_supplies$score)
barplot(score_frequencies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
Temp <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
tc$code_dictionary(Temp, column='dict_sentiment')
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
View(Temp)
tc$code_dictionary(Temp, column='dict_sentiment')
View(Temp)
View(Temp)
summary(Temp)
typeof(Temp)
Temp <- AuthoritatianDict[AuthoritatianDict$code != "rwa",]
tc$code_dictionary(Temp, column='dict_sentiment')
Temp <- AuthoritatianDict[AuthoritatianDict$code != "lwa",]
tc$code_dictionary(Temp, column='dict_sentiment')
Temp <- AuthoritatianDict[AuthoritatianDict$code == "lwa",]
tc$code_dictionary(Temp, column='dict_sentiment')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code == "lwa",]
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
score_frequencies = table(pet_supplies$score)
barplot(score_frequencies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
score_frequencies = table(pet_supplies$score)
barplot(score_frequencies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
tc
tc$meta
tc$tokens$dict_sentiment
View(AuthoritatianDict)
tc$tokens$code
tc$tokens$token
tc$tokens$dict_sentiment_id
AuthoritatianDict <- AuthoritatianDict[1:9,]
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
AuthoritatianDict <- AuthoritatianDict[1:5,]
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
score_frequencies = table(pet_supplies$score)
barplot(score_frequencies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
?tc$code_dictionary
?tc
?create_tcorpus
?code_dictionary
#AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
quanteda::as.dictionary(AuthoritatianDict)
#AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
colnames(AuthoritatianDict) <- c("word","sentiment")
View(AuthoritatianDict)
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
#AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
colnames(AuthoritatianDict) <- c("word","sentiment")
quanteda::as.dictionary(AuthoritatianDict)
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
AuthoritatianDict <- quanteda::as.dictionary(AuthoritatianDict)
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
View(AuthoritatianDict)
View(AuthoritatianDict)
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
View(AuthoritatianDict)
AuthoritatianDict$code == "*rwa"
sume(AuthoritatianDict$code == "*rwa")
sum(AuthoritatianDict$code == "*rwa")
View(AuthoritatianDict)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
#AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
AuthoritatianDict <- complete.cases(AuthoritatianDict)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
#AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
AuthoritatianDict <- AuthoritatianDict[complete.cases(AuthoritatianDict),]
colnames(AuthoritatianDict) <- c("word","sentiment")
AuthoritatianDict <- quanteda::as.dictionary(AuthoritatianDict)
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
sum(AuthoritatianDict$code == "*rwa")
sum(AuthoritatianDict$code == "* rwa")
View(AuthoritatianDict)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
AuthoritatianDict <- AuthoritatianDict[complete.cases(AuthoritatianDict),]
colnames(AuthoritatianDict) <- c("word","sentiment")
AuthoritatianDict <- quanteda::as.dictionary(AuthoritatianDict)
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
sent_counts$dict_score = log(sent_counts$lwa + 0.5) - log(sent_counts$rwa + 0.5)
hist(sent_counts$dict_score, xlab='score', main='dictionary sentiment score')
View(sent_counts)
library(readr)
library(corpustools)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.dictionaries)
#########
#########
pet_supplies <- read_csv("~/VU Teaching/SMA 2021/data/Pet_Supplies_5.csv")
score_frequencies = table(pet_supplies$score)
barplot(score_frequencies)
##################
# Rule based sentiment analysis in R
##################
tc = create_tcorpus(pet_supplies, doc_column = 'id', text_columns = 'text')
AuthoritatianDict = read_csv('~/Desktop/Authoritarian dictionary - ALLES.csv')
AuthoritatianDict$dict_sentiment = AuthoritatianDict$code
AuthoritatianDict <- AuthoritatianDict[AuthoritatianDict$code != "aut",]
AuthoritatianDict <- AuthoritatianDict[complete.cases(AuthoritatianDict),]
colnames(AuthoritatianDict) <- c("word","sentiment")
AuthoritatianDict <- quanteda::as.dictionary(AuthoritatianDict)
tc$code_dictionary(AuthoritatianDict, column='dict_sentiment')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='dict_sentiment')
##################
# Calculating a sentiment score
##################
sent_counts = count_tcorpus(tc, meta_cols = c('doc_id', 'score'),
feature='dict_sentiment', count = 'tokens')
sent_counts
sent_counts$dict_score = log(sent_counts$lwa + 0.5) - log(sent_counts$rwa + 0.5)
hist(sent_counts$dict_score, xlab='score', main='dictionary sentiment score')
sent_counts$dict_score = log(sent_counts$lwa + 0.5) - log(sent_counts$rwa + 0.5)
View(sent_counts)
library(readr)
AFINN_dictionary <- read_csv("Downloads/AFINN_dictionary.csv")
View(AFINN_dictionary)
library(readr)
Dogecoin2weeksbefore <- read_delim("Downloads/Dogecoin2weeksbefore.csv",
";", escape_double = FALSE, trim_ws = TRUE)
View(Dogecoin2weeksbefore)
library(ggplot2)
library(readr)
library(stm)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(corpustools)
library(quanteda.dictionaries)
#d = read.csv2('Dogecoin2weeksbefore.csv')
d <- Dogecoin2weeksbefore
d$id <- 1:nrow(d)
tc <- create_tcorpus(d, doc_column = 'id', text_columns = 'Message')
tc$code_dictionary(AFINN, column ='code')
AFINN <- AFINN_dictionary
d$id <- 1:nrow(d)
tc <- create_tcorpus(d, doc_column = 'id', text_columns = 'Message')
tc$code_dictionary(AFINN, column ='code')
head(tc$tokens, 50)
browse_texts(tc, n = 10, category='code')
sent_counts = count_tcorpus(tc , meta_cols = c('doc_id'),feature='code', count = 'tokens')
sent_counts$dict_score = log(sent_counts$positive + 0.5) - log(sent_counts$negative + 0.5)
hist(sent_counts$dict_score, xlab = 'score', main = 'dictionary sentiment score')
`15to21` <- read.csv("~/Downloads/15to21.csv", sep=";")
View(`15to21`)
Week7to14 <- read.csv("~/Downloads/Week7to14.csv", sep=";")
View(Week7to14)
Week7to14.1 <- read.csv("~/Downloads/Week7to14-1.csv", sep=";")
View(Week7to14.1)
## calculate precision and recall
precision = diag(cm) / colSums(cm)
library(readr)
PImPo_qsl_wo_verbatim <- read_csv("Desktop/PImPo_qsl_wo_verbatim.csv")
View(PImPo_qsl_wo_verbatim)
library(readr)
PImPo_party <- read_csv("Desktop/PImPo_party.csv")
View(PImPo_party)
# Install required packages (only necessary if not already installed)
install.packages("magrittr")
install.packages("dplyr")
install.packages("manifestoR")
install.packages("haven")
# Load packages
library(magrittr)
library(dplyr)
library(manifestoR)
library(haven)
# Set working directory
setwd("....") # Substitute .... with the file path of the directory in which you are working,
# Set working directory
setwd("Desktop/Plm Manifest/") # Substitute .... with the file path of the directory in which you are working,
# Sign up on the Manifesto Project website (https://manifesto-project.wzb.eu/)
# and download your personal API key.
# Save it into the working directory set above.
mp_setapikey(key.file = "manifesto_apikey.txt")
mp_use_corpus_version(versionid = 20150708174629)
# Add the data set to your R session.
PImPo <- read.csv("PImPo_qsl_wo_verbatim.csv", sep = ",", dec = ".", stringsAsFactors = F, fileEncoding = "UTF-8", encoding = "UTF-8")
# Add the data set to your R session.
PImPo <- read.csv("PImPo_qsl_wo_verbatim.csv", sep = ",", dec = ".", stringsAsFactors = F, fileEncoding = "UTF-8", encoding = "UTF-8")
# Access verbatim from corpus
documents <- PImPo %>% group_by(party, date) %>% slice(1) %>% ungroup()
corpus <- mp_metadata(documents) %>%
subset(annotations) %>%
mp_corpus() %>%
as.data.frame(with.meta = TRUE) %>%
filter(text != ".") %>%
mutate(country = substr(as.character(party), 1, 2)) %>%
mutate(country = as.integer(country)) %>%
select(content = text, party, date, country, cmp_code, pos_corpus = pos) %>%
filter(!is.na(cmp_code)) %>%
mutate(party = as.integer(party), date = as.integer(date))
# Add verbatim to the immigration dataset
PImPo_preproc_verbatim <- left_join(PImPo, corpus %>% select(date, party, pos_corpus, cmp_code, content),
by = c("date", "party", "pos_corpus"))
# The 2007 manifesto of the Finish National Coalition
# and the 2013 manifesto of the German Greens in the corpus
# does not contain all verbatim that was coded by the crowd.
# Execute the next line, to see how many quasi-sentence are affected by this.
PImPo_preproc_verbatim %>% filter(is.na(content)) %$% table(party, date)
# For excluding the specific quasi-sentences run this line.
PImPo_verbatim <- PImPo_preproc_verbatim %>% filter(!is.na(pos_corpus))
# If you want to exclude the marginal text from the PImPo, continue the script in line 74.
# For adding the text execute the following lines, too.
mis_verbatim <- read.csv("mis_verbatim.csv", sep = ",", dec = ".", stringsAsFactors = F, fileEncoding = "UTF-8", encoding = "UTF-8")
PImPo_mis_verbatim <- PImPo_preproc_verbatim %>% filter(is.na(pos_corpus))
cleaned_PImPo_mis_verbatim <- left_join(PImPo_mis_verbatim %>% select(-c(content, cmp_code)),
mis_verbatim,
by = c("rn"))
PImPo_verbatim <- rbind(PImPo_verbatim, cleaned_PImPo_mis_verbatim)
PImPo_verbatim <- PImPo_verbatim %>% arrange(rn)
# Save dataset with verbatim to your working directory
# If you want to easily open the csv later in xlsx,
# it is useful to set the correct column seperator now.
# Which one you need depends on the language installations on your computer,
# in most cases the "," will be correct, if that is not
# the case it will be the semicolon (see second line).
# If you want all special characters displayed correctly in Excel, make sure
# to specify the encoding as UTF-8 when reading the csv into Excel.
write.table(PImPo_verbatim, "PImPo_verbatim.csv", fileEncoding = "UTF-8", row.names = FALSE, sep = ",")
View(PImPo_verbatim)
View(PImPo_verbatim)
filter(PImPo_verbatim,country,"22")
filter(PImPo_verbatim,country,22)
?filter
PImPo_verbatim %>% filter(country=22)
PImPo_verbatim %>% filter(country='22')
filter(PImPo_verbatim,country==22)
NLData <- filter(PImPo_verbatim,country==22)
View(NLData)
summary(NLData)
names(NLData)
